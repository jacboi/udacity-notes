PCA principle component analysis

transforms your coordinate system to simply the dimensions

1 moves the centroid of the current coord system to the centroid of data
2 rotates x axis with principle axis of variation 
3 generates weights in terms of variation in each axis
    - generated by eigen value decomposition



PCA will always return values
analysis of the eigen decomp is necessary to evaluate validity


measurable vs latent features
selectkbest = sklearn feature selection tool
    - keeps k best features
selectpercentile = sklearn 
    - keeps percentage of features

composite features
    - core aspect of PCA

how to determine the principal component
1 fit centroid
2 align axis
3 evaluate longest axis for variance
    - variance : roughly spread of data - similar to standard deviation
4 compress the data
    - involves projection step onto selected axis
    - data loss is represented by the summed distance of this projection

PCA : really useful for reducing dimensions of a given set of data
    - helps to understand features about the patterns and relationships between features in data
    - systematized way to transfrom input features into principle components or composite features
    - directions in data that maximize variance
    - there is not overlapping aspects of the 

when to use PCA
    - when you think or there are latent features
    - dimensionality reduction
        - visualize high dimensional data
        - reduce noise in data
        - make other algos (regression, classification) work better b/c fewer inputs
            - ie eigen faces