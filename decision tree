"""
-------
ENTROPY
-------
 a concept for describing error in a decision tree


 average product of probablilty log2

 ie for 8 red balls, 2 yellow balls and 3 blue balls
 
 """

import math

pr = 8/13
py = 2/13
pb = 3/13

En = -pr*math.log2(pr) - py*math.log2(py) - pb*math.log2(pb)

print(En)

"""
there is an ENTROPY of 1.3346791410515946
"""

"""
----------------
INFORMATION GAIN
----------------
the difference of the parent and the average entropy of the children


"""

"""
-------------
RANDOM FOREST
-------------
picking random collections of features and calculating error

decision trees have a tendency to overfit and memorize data
radom forests are a method for dealing with this

"""

"""
-------
HYPERPARAMETERS FOR DECISION TREES
----------------------------------
Maximum Depth
The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. 

Minimum number of samples to split
A node must have at least min_samples_split samples in order to be large enough to split.

Minimum number of samples per leaf
When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other.

